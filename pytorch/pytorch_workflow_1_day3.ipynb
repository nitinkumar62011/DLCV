{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import matplotlib.pyplot as plt\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prepation and \n",
    "# Imn Ml we have to perform 2 things\n",
    "  1 Get Data into numerical representaion<p>\n",
    "  2. Build a model to learn the pattern on that numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=0.7\n",
    "bias=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.arange(1,200,0.1).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1990, 1]), torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,x.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7734375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(x)*4)/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.77 KB'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_size(len(x)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=weight*x+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0000],\n",
       "        [  1.0700],\n",
       "        [  1.1400],\n",
       "        ...,\n",
       "        [140.0900],\n",
       "        [140.1600],\n",
       "        [140.2300]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 1990)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# data=pd.DataFrame({'x':[i  for i in x]})\n",
    "# data.to_csv(\"x_data2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1592"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training and testing data set\n",
    "train_split=int(0.8*len(x))\n",
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=x[:train_split],y[:train_split]\n",
    "x_test,y_test=x[train_split:],y[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592, 398, 1592, 398)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(x_test),len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a function to visualize the data\n",
    "def plot_prediction(train_data=x_train,train_label=y_train,test_data=x_test,test_label=y_test,predictions=None):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(train_data,train_label,c='b',s=4,label=\"training_data\")\n",
    "    plt.scatter(test_data,test_label,c='g',s=4,label=\"Testing data\")\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data,predictions,c=\"r\",label=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating model for linear regression\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w=nn.Parameter(torch.randn(1,dtype=torch.float),requires_grad=True)\n",
    "        self.b=nn.Parameter(torch.randn(1,dtype=torch.float),requires_grad=True)\n",
    "    def forward(self,x:torch.Tensor) ->torch.Tensor:\n",
    "        return self.w*x+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "model_0=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model_0(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(predictions=pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating loss function\n",
    "loss_fn=nn.L1Loss()\n",
    "#setup optimizer\n",
    "optimizer=torch.optim.SGD(params=model_0.parameters(),lr=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('w', tensor([-0.6484])), ('b', tensor([-0.7058]))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 998 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-104.5797])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### making prediction using torch.inference_model()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_pred=model_0(x_test)\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(109.6195, grad_fn=<MeanBackward0>)\n",
      "loss tensor(5730.7529, grad_fn=<MeanBackward0>)\n",
      "loss tensor(109.6194, grad_fn=<MeanBackward0>)\n",
      "loss tensor(5730.7529, grad_fn=<MeanBackward0>)\n",
      "loss tensor(109.6194, grad_fn=<MeanBackward0>)\n",
      "loss tensor(5730.7529, grad_fn=<MeanBackward0>)\n",
      "loss tensor(109.6194, grad_fn=<MeanBackward0>)\n",
      "loss tensor(5730.7529, grad_fn=<MeanBackward0>)\n",
      "loss tensor(109.6194, grad_fn=<MeanBackward0>)\n",
      "loss tensor(5730.7529, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "##Training\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    #forward pass\n",
    "    y_pred=model_0(x_train)\n",
    "    #calculating the loss\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    print(\"loss\",loss)\n",
    "    # optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # backword on the loss wrt parameter\n",
    "    loss.backward()\n",
    "    #step to optimizer\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss() \u001b[38;5;66;03m# MAE loss is same as L1Loss\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create the optimizer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m(params\u001b[38;5;241m=\u001b[39mmodel_0\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;66;03m# parameters of target model to optimize\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                             lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.09\u001b[39m) \u001b[38;5;66;03m# learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'adam'"
     ]
    }
   ],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.adam(params=model_0.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.09) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 10 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 20 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 30 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 40 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 50 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 60 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 70 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 80 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 90 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 1990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 2990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 3990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 4990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 5990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 6990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 7990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 8990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9000 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9010 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9020 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9030 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9040 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9050 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9060 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9070 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9080 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9090 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9100 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9110 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9120 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9130 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9140 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9150 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9160 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9170 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9180 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9190 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9200 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9210 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9220 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9230 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9240 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9250 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9260 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9270 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9280 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9290 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9300 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9310 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9320 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9330 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9340 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9350 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9360 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9370 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9380 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9390 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9400 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9410 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9420 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9430 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9440 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9450 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9460 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9470 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9480 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9490 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9500 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9510 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9520 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9530 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9540 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9550 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9560 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9570 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9580 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9590 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9600 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9610 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9620 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9630 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9640 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9650 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9660 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9670 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9680 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9690 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9700 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9710 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9720 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9730 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9740 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9750 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9760 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9770 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9780 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9790 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9800 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9810 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9820 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9830 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9840 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9850 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9860 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9870 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9880 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9890 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9900 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9910 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9920 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9930 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9940 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9950 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9960 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9970 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9980 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n",
      "Epoch: 9990 | MAE Train Loss: 109.6194076538086 | MAE Test Loss: 12809.8408203125 \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs (how many times the model will pass over the training data)\n",
    "epochs = 10000\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "\n",
    "    # Put model in training mode (this is the default state of a model)\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_0(x_train)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_0(x_test)\n",
    "\n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(predictions=model_0(x_test).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
