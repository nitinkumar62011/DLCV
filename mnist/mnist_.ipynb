{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\hp\\Desktop\\DLCV\\mnist\\env\n",
      "\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate c:\\Users\\hp\\Desktop\\DLCV\\mnist\\env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 24.3.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.3.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda create -p env -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate c:\\Users\\hp\\Desktop\\DLCV\\mnist\\env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "Installing collected packages: tensorflow-intel, tensorflow\n",
      "Successfully installed tensorflow-2.16.1 tensorflow-intel-2.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import  Dense,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "data=keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x242e60e3850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train/255\n",
    "x_test=x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x242e61ca250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,938</span> (409.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,938\u001b[0m (409.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,938</span> (409.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,938\u001b[0m (409.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0608 - val_accuracy: 0.9717 - val_loss: 0.0970\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0545 - val_accuracy: 0.9709 - val_loss: 0.0988\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9854 - loss: 0.0547 - val_accuracy: 0.9718 - val_loss: 0.0957\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0516 - val_accuracy: 0.9730 - val_loss: 0.0939\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0525 - val_accuracy: 0.9732 - val_loss: 0.0929\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0436 - val_accuracy: 0.9739 - val_loss: 0.0911\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0443 - val_accuracy: 0.9747 - val_loss: 0.0904\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0420 - val_accuracy: 0.9741 - val_loss: 0.0915\n",
      "Epoch 9/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0385 - val_accuracy: 0.9737 - val_loss: 0.0920\n",
      "Epoch 10/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9907 - loss: 0.0379 - val_accuracy: 0.9743 - val_loss: 0.0907\n",
      "Epoch 11/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0352 - val_accuracy: 0.9743 - val_loss: 0.0922\n",
      "Epoch 12/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0350 - val_accuracy: 0.9743 - val_loss: 0.0909\n",
      "Epoch 13/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0315 - val_accuracy: 0.9751 - val_loss: 0.0904\n",
      "Epoch 14/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0300 - val_accuracy: 0.9757 - val_loss: 0.0896\n",
      "Epoch 15/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0298 - val_accuracy: 0.9755 - val_loss: 0.0892\n",
      "Epoch 16/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0268 - val_accuracy: 0.9757 - val_loss: 0.0880\n",
      "Epoch 17/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0276 - val_accuracy: 0.9758 - val_loss: 0.0885\n",
      "Epoch 18/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0242 - val_accuracy: 0.9737 - val_loss: 0.0938\n",
      "Epoch 19/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0227 - val_accuracy: 0.9765 - val_loss: 0.0891\n",
      "Epoch 20/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0224 - val_accuracy: 0.9760 - val_loss: 0.0902\n",
      "Epoch 21/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0222 - val_accuracy: 0.9754 - val_loss: 0.0903\n",
      "Epoch 22/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0213 - val_accuracy: 0.9760 - val_loss: 0.0890\n",
      "Epoch 23/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0206 - val_accuracy: 0.9765 - val_loss: 0.0888\n",
      "Epoch 24/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9963 - loss: 0.0195 - val_accuracy: 0.9756 - val_loss: 0.0899\n",
      "Epoch 25/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0182 - val_accuracy: 0.9758 - val_loss: 0.0899\n",
      "Epoch 26/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9966 - loss: 0.0181 - val_accuracy: 0.9758 - val_loss: 0.0905\n",
      "Epoch 27/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0169 - val_accuracy: 0.9753 - val_loss: 0.0914\n",
      "Epoch 28/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0158 - val_accuracy: 0.9742 - val_loss: 0.0928\n",
      "Epoch 29/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0150 - val_accuracy: 0.9758 - val_loss: 0.0911\n",
      "Epoch 30/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0148 - val_accuracy: 0.9761 - val_loss: 0.0903\n",
      "Epoch 31/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0150 - val_accuracy: 0.9759 - val_loss: 0.0913\n",
      "Epoch 32/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0145 - val_accuracy: 0.9754 - val_loss: 0.0930\n",
      "Epoch 33/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0131 - val_accuracy: 0.9755 - val_loss: 0.0929\n",
      "Epoch 34/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0123 - val_accuracy: 0.9751 - val_loss: 0.0949\n",
      "Epoch 35/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0130 - val_accuracy: 0.9748 - val_loss: 0.0939\n",
      "Epoch 36/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0110 - val_accuracy: 0.9754 - val_loss: 0.0946\n",
      "Epoch 37/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0107 - val_accuracy: 0.9757 - val_loss: 0.0938\n",
      "Epoch 38/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0107 - val_accuracy: 0.9758 - val_loss: 0.0937\n",
      "Epoch 39/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0104 - val_accuracy: 0.9755 - val_loss: 0.0950\n",
      "Epoch 40/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0102 - val_accuracy: 0.9749 - val_loss: 0.0963\n",
      "Epoch 41/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0101 - val_accuracy: 0.9761 - val_loss: 0.0946\n",
      "Epoch 42/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0098 - val_accuracy: 0.9754 - val_loss: 0.0967\n",
      "Epoch 43/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0091 - val_accuracy: 0.9762 - val_loss: 0.0960\n",
      "Epoch 44/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0084 - val_accuracy: 0.9764 - val_loss: 0.0953\n",
      "Epoch 45/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0083 - val_accuracy: 0.9749 - val_loss: 0.0977\n",
      "Epoch 46/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0080 - val_accuracy: 0.9747 - val_loss: 0.0975\n",
      "Epoch 47/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0076 - val_accuracy: 0.9752 - val_loss: 0.0975\n",
      "Epoch 48/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0079 - val_accuracy: 0.9748 - val_loss: 0.0993\n",
      "Epoch 49/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0070 - val_accuracy: 0.9761 - val_loss: 0.0976\n",
      "Epoch 50/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0069 - val_accuracy: 0.9751 - val_loss: 0.0983\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.9836666584014893,\n",
       "  0.9850208163261414,\n",
       "  0.9854583144187927,\n",
       "  0.9861249923706055,\n",
       "  0.9869375228881836,\n",
       "  0.9877708554267883,\n",
       "  0.9885625243186951,\n",
       "  0.9890000224113464,\n",
       "  0.9900624752044678,\n",
       "  0.9904166460037231,\n",
       "  0.9907291531562805,\n",
       "  0.9914583563804626,\n",
       "  0.991895854473114,\n",
       "  0.9925000071525574,\n",
       "  0.9928749799728394,\n",
       "  0.9934583306312561,\n",
       "  0.9942916631698608,\n",
       "  0.9941874742507935,\n",
       "  0.9947916865348816,\n",
       "  0.9950624704360962,\n",
       "  0.995437502861023,\n",
       "  0.9958124756813049,\n",
       "  0.9960416555404663,\n",
       "  0.9961875081062317,\n",
       "  0.9965833425521851,\n",
       "  0.996666669845581,\n",
       "  0.997041642665863,\n",
       "  0.9973333477973938,\n",
       "  0.9976041913032532,\n",
       "  0.9976666569709778,\n",
       "  0.9978125095367432,\n",
       "  0.9979583621025085,\n",
       "  0.9980208277702332,\n",
       "  0.9982083439826965,\n",
       "  0.9984999895095825,\n",
       "  0.9986249804496765,\n",
       "  0.9986666440963745,\n",
       "  0.9988750219345093,\n",
       "  0.9988958239555359,\n",
       "  0.9991250038146973,\n",
       "  0.9991458058357239,\n",
       "  0.9993333220481873,\n",
       "  0.9992708563804626,\n",
       "  0.9992708563804626,\n",
       "  0.9993958473205566,\n",
       "  0.9994166493415833,\n",
       "  0.9994791746139526,\n",
       "  0.9994583129882812,\n",
       "  0.9995833039283752,\n",
       "  0.9995833039283752],\n",
       " 'loss': [0.05905584618449211,\n",
       "  0.056057579815387726,\n",
       "  0.053246937692165375,\n",
       "  0.05065139755606651,\n",
       "  0.04813125729560852,\n",
       "  0.045873358845710754,\n",
       "  0.04394957423210144,\n",
       "  0.04172752797603607,\n",
       "  0.039431873708963394,\n",
       "  0.03789082169532776,\n",
       "  0.03622445836663246,\n",
       "  0.03436267748475075,\n",
       "  0.03299116715788841,\n",
       "  0.03133247792720795,\n",
       "  0.03015902265906334,\n",
       "  0.0286868829280138,\n",
       "  0.02721947245299816,\n",
       "  0.025960184633731842,\n",
       "  0.02485152706503868,\n",
       "  0.023913007229566574,\n",
       "  0.022752443328499794,\n",
       "  0.021627821028232574,\n",
       "  0.020720355212688446,\n",
       "  0.01988861709833145,\n",
       "  0.018915362656116486,\n",
       "  0.018375910818576813,\n",
       "  0.017487546429038048,\n",
       "  0.016619408503174782,\n",
       "  0.015898678451776505,\n",
       "  0.015243498608469963,\n",
       "  0.014643174596130848,\n",
       "  0.014078320004045963,\n",
       "  0.013456315733492374,\n",
       "  0.012920776382088661,\n",
       "  0.012455706484615803,\n",
       "  0.011963523924350739,\n",
       "  0.011451748199760914,\n",
       "  0.010986986570060253,\n",
       "  0.010646793991327286,\n",
       "  0.010137234814465046,\n",
       "  0.009741473011672497,\n",
       "  0.009505681693553925,\n",
       "  0.009138571098446846,\n",
       "  0.008675205521285534,\n",
       "  0.00831591710448265,\n",
       "  0.008156243711709976,\n",
       "  0.007826419547200203,\n",
       "  0.007588609587401152,\n",
       "  0.007278819568455219,\n",
       "  0.007081265095621347],\n",
       " 'val_accuracy': [0.971666693687439,\n",
       "  0.9709166884422302,\n",
       "  0.971833348274231,\n",
       "  0.9729999899864197,\n",
       "  0.9731666445732117,\n",
       "  0.9739166498184204,\n",
       "  0.9747499823570251,\n",
       "  0.9740833044052124,\n",
       "  0.9736666679382324,\n",
       "  0.9742500185966492,\n",
       "  0.9742500185966492,\n",
       "  0.9742500185966492,\n",
       "  0.9750833511352539,\n",
       "  0.9756666421890259,\n",
       "  0.9754999876022339,\n",
       "  0.9756666421890259,\n",
       "  0.9757500290870667,\n",
       "  0.9737499952316284,\n",
       "  0.9764999747276306,\n",
       "  0.9760000109672546,\n",
       "  0.9754166603088379,\n",
       "  0.9760000109672546,\n",
       "  0.9764999747276306,\n",
       "  0.9755833148956299,\n",
       "  0.9757500290870667,\n",
       "  0.9757500290870667,\n",
       "  0.9752500057220459,\n",
       "  0.9741666913032532,\n",
       "  0.9757500290870667,\n",
       "  0.9760833382606506,\n",
       "  0.9759166836738586,\n",
       "  0.9754166603088379,\n",
       "  0.9754999876022339,\n",
       "  0.9750833511352539,\n",
       "  0.9748333096504211,\n",
       "  0.9754166603088379,\n",
       "  0.9756666421890259,\n",
       "  0.9757500290870667,\n",
       "  0.9754999876022339,\n",
       "  0.9749166369438171,\n",
       "  0.9760833382606506,\n",
       "  0.9754166603088379,\n",
       "  0.9762499928474426,\n",
       "  0.9764166474342346,\n",
       "  0.9749166369438171,\n",
       "  0.9747499823570251,\n",
       "  0.9751666784286499,\n",
       "  0.9748333096504211,\n",
       "  0.9760833382606506,\n",
       "  0.9750833511352539],\n",
       " 'val_loss': [0.09695102274417877,\n",
       "  0.09877916425466537,\n",
       "  0.09571662545204163,\n",
       "  0.09392239153385162,\n",
       "  0.09290923178195953,\n",
       "  0.09106114506721497,\n",
       "  0.0904277116060257,\n",
       "  0.09145602583885193,\n",
       "  0.09202603995800018,\n",
       "  0.0906941145658493,\n",
       "  0.0921526774764061,\n",
       "  0.0909181535243988,\n",
       "  0.09038656949996948,\n",
       "  0.08958031982183456,\n",
       "  0.08915144950151443,\n",
       "  0.08800891786813736,\n",
       "  0.08846589177846909,\n",
       "  0.09379472583532333,\n",
       "  0.08906521648168564,\n",
       "  0.09021318703889847,\n",
       "  0.09026654809713364,\n",
       "  0.0889531672000885,\n",
       "  0.08883821964263916,\n",
       "  0.08989696204662323,\n",
       "  0.08988655358552933,\n",
       "  0.09045965224504471,\n",
       "  0.09143339097499847,\n",
       "  0.09278429299592972,\n",
       "  0.09112163633108139,\n",
       "  0.09032809734344482,\n",
       "  0.09130693227052689,\n",
       "  0.09302297234535217,\n",
       "  0.0929410308599472,\n",
       "  0.09488335251808167,\n",
       "  0.09390807896852493,\n",
       "  0.09458470344543457,\n",
       "  0.09383241087198257,\n",
       "  0.09374470263719559,\n",
       "  0.09502414613962173,\n",
       "  0.09631774574518204,\n",
       "  0.09464511275291443,\n",
       "  0.0966874212026596,\n",
       "  0.09603729099035263,\n",
       "  0.09530258923768997,\n",
       "  0.09766967594623566,\n",
       "  0.09751622378826141,\n",
       "  0.09748613834381104,\n",
       "  0.0993197113275528,\n",
       "  0.09760413318872452,\n",
       "  0.09826279431581497]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss+accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.983667</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>0.096951</td>\n",
       "      <td>1.042723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.985021</td>\n",
       "      <td>0.056058</td>\n",
       "      <td>0.970917</td>\n",
       "      <td>0.098779</td>\n",
       "      <td>1.041078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.053247</td>\n",
       "      <td>0.971833</td>\n",
       "      <td>0.095717</td>\n",
       "      <td>1.038705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.986125</td>\n",
       "      <td>0.050651</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>0.093922</td>\n",
       "      <td>1.036776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.986938</td>\n",
       "      <td>0.048131</td>\n",
       "      <td>0.973167</td>\n",
       "      <td>0.092909</td>\n",
       "      <td>1.035069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.987771</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>0.973917</td>\n",
       "      <td>0.091061</td>\n",
       "      <td>1.033644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.988563</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>0.974750</td>\n",
       "      <td>0.090428</td>\n",
       "      <td>1.032512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.989000</td>\n",
       "      <td>0.041728</td>\n",
       "      <td>0.974083</td>\n",
       "      <td>0.091456</td>\n",
       "      <td>1.030728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.039432</td>\n",
       "      <td>0.973667</td>\n",
       "      <td>0.092026</td>\n",
       "      <td>1.029494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990417</td>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.974250</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>1.028307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.990729</td>\n",
       "      <td>0.036224</td>\n",
       "      <td>0.974250</td>\n",
       "      <td>0.092153</td>\n",
       "      <td>1.026954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.991458</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>0.974250</td>\n",
       "      <td>0.090918</td>\n",
       "      <td>1.025821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.991896</td>\n",
       "      <td>0.032991</td>\n",
       "      <td>0.975083</td>\n",
       "      <td>0.090387</td>\n",
       "      <td>1.024887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.089580</td>\n",
       "      <td>1.023832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.992875</td>\n",
       "      <td>0.030159</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.089151</td>\n",
       "      <td>1.023034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.088009</td>\n",
       "      <td>1.022145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.994292</td>\n",
       "      <td>0.027219</td>\n",
       "      <td>0.975750</td>\n",
       "      <td>0.088466</td>\n",
       "      <td>1.021511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.994187</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.973750</td>\n",
       "      <td>0.093795</td>\n",
       "      <td>1.020148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>0.089065</td>\n",
       "      <td>1.019643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.090213</td>\n",
       "      <td>1.018975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.995438</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.975417</td>\n",
       "      <td>0.090267</td>\n",
       "      <td>1.018190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.995812</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.088953</td>\n",
       "      <td>1.017440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.996042</td>\n",
       "      <td>0.020720</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>0.088838</td>\n",
       "      <td>1.016762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.996188</td>\n",
       "      <td>0.019889</td>\n",
       "      <td>0.975583</td>\n",
       "      <td>0.089897</td>\n",
       "      <td>1.016076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.996583</td>\n",
       "      <td>0.018915</td>\n",
       "      <td>0.975750</td>\n",
       "      <td>0.089887</td>\n",
       "      <td>1.015499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.975750</td>\n",
       "      <td>0.090460</td>\n",
       "      <td>1.015043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.997042</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.975250</td>\n",
       "      <td>0.091433</td>\n",
       "      <td>1.014529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>0.974167</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>1.013953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.997604</td>\n",
       "      <td>0.015899</td>\n",
       "      <td>0.975750</td>\n",
       "      <td>0.091122</td>\n",
       "      <td>1.013503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.997667</td>\n",
       "      <td>0.015243</td>\n",
       "      <td>0.976083</td>\n",
       "      <td>0.090328</td>\n",
       "      <td>1.012910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.997813</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.975917</td>\n",
       "      <td>0.091307</td>\n",
       "      <td>1.012456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.997958</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.975417</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>1.012037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.998021</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.092941</td>\n",
       "      <td>1.011477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.998208</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>0.975083</td>\n",
       "      <td>0.094883</td>\n",
       "      <td>1.011129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.012456</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>0.093908</td>\n",
       "      <td>1.010956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.998625</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.975417</td>\n",
       "      <td>0.094585</td>\n",
       "      <td>1.010589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.011452</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>1.010118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.998875</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.975750</td>\n",
       "      <td>0.093745</td>\n",
       "      <td>1.009862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.998896</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.095024</td>\n",
       "      <td>1.009543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>0.974917</td>\n",
       "      <td>0.096318</td>\n",
       "      <td>1.009262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.999146</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.976083</td>\n",
       "      <td>0.094645</td>\n",
       "      <td>1.008887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.999333</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.975417</td>\n",
       "      <td>0.096687</td>\n",
       "      <td>1.008839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.976250</td>\n",
       "      <td>0.096037</td>\n",
       "      <td>1.008409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.976417</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>1.007946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.974917</td>\n",
       "      <td>0.097670</td>\n",
       "      <td>1.007712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>0.974750</td>\n",
       "      <td>0.097516</td>\n",
       "      <td>1.007573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.975167</td>\n",
       "      <td>0.097486</td>\n",
       "      <td>1.007306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.999458</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>0.099320</td>\n",
       "      <td>1.007047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.007279</td>\n",
       "      <td>0.976083</td>\n",
       "      <td>0.097604</td>\n",
       "      <td>1.006862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.975083</td>\n",
       "      <td>0.098263</td>\n",
       "      <td>1.006665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy      loss  val_accuracy  val_loss  loss+accuracy\n",
       "0   0.983667  0.059056      0.971667  0.096951       1.042723\n",
       "1   0.985021  0.056058      0.970917  0.098779       1.041078\n",
       "2   0.985458  0.053247      0.971833  0.095717       1.038705\n",
       "3   0.986125  0.050651      0.973000  0.093922       1.036776\n",
       "4   0.986938  0.048131      0.973167  0.092909       1.035069\n",
       "5   0.987771  0.045873      0.973917  0.091061       1.033644\n",
       "6   0.988563  0.043950      0.974750  0.090428       1.032512\n",
       "7   0.989000  0.041728      0.974083  0.091456       1.030728\n",
       "8   0.990062  0.039432      0.973667  0.092026       1.029494\n",
       "9   0.990417  0.037891      0.974250  0.090694       1.028307\n",
       "10  0.990729  0.036224      0.974250  0.092153       1.026954\n",
       "11  0.991458  0.034363      0.974250  0.090918       1.025821\n",
       "12  0.991896  0.032991      0.975083  0.090387       1.024887\n",
       "13  0.992500  0.031332      0.975667  0.089580       1.023832\n",
       "14  0.992875  0.030159      0.975500  0.089151       1.023034\n",
       "15  0.993458  0.028687      0.975667  0.088009       1.022145\n",
       "16  0.994292  0.027219      0.975750  0.088466       1.021511\n",
       "17  0.994187  0.025960      0.973750  0.093795       1.020148\n",
       "18  0.994792  0.024852      0.976500  0.089065       1.019643\n",
       "19  0.995062  0.023913      0.976000  0.090213       1.018975\n",
       "20  0.995438  0.022752      0.975417  0.090267       1.018190\n",
       "21  0.995812  0.021628      0.976000  0.088953       1.017440\n",
       "22  0.996042  0.020720      0.976500  0.088838       1.016762\n",
       "23  0.996188  0.019889      0.975583  0.089897       1.016076\n",
       "24  0.996583  0.018915      0.975750  0.089887       1.015499\n",
       "25  0.996667  0.018376      0.975750  0.090460       1.015043\n",
       "26  0.997042  0.017488      0.975250  0.091433       1.014529\n",
       "27  0.997333  0.016619      0.974167  0.092784       1.013953\n",
       "28  0.997604  0.015899      0.975750  0.091122       1.013503\n",
       "29  0.997667  0.015243      0.976083  0.090328       1.012910\n",
       "30  0.997813  0.014643      0.975917  0.091307       1.012456\n",
       "31  0.997958  0.014078      0.975417  0.093023       1.012037\n",
       "32  0.998021  0.013456      0.975500  0.092941       1.011477\n",
       "33  0.998208  0.012921      0.975083  0.094883       1.011129\n",
       "34  0.998500  0.012456      0.974833  0.093908       1.010956\n",
       "35  0.998625  0.011964      0.975417  0.094585       1.010589\n",
       "36  0.998667  0.011452      0.975667  0.093832       1.010118\n",
       "37  0.998875  0.010987      0.975750  0.093745       1.009862\n",
       "38  0.998896  0.010647      0.975500  0.095024       1.009543\n",
       "39  0.999125  0.010137      0.974917  0.096318       1.009262\n",
       "40  0.999146  0.009741      0.976083  0.094645       1.008887\n",
       "41  0.999333  0.009506      0.975417  0.096687       1.008839\n",
       "42  0.999271  0.009139      0.976250  0.096037       1.008409\n",
       "43  0.999271  0.008675      0.976417  0.095303       1.007946\n",
       "44  0.999396  0.008316      0.974917  0.097670       1.007712\n",
       "45  0.999417  0.008156      0.974750  0.097516       1.007573\n",
       "46  0.999479  0.007826      0.975167  0.097486       1.007306\n",
       "47  0.999458  0.007589      0.974833  0.099320       1.007047\n",
       "48  0.999583  0.007279      0.976083  0.097604       1.006862\n",
       "49  0.999583  0.007081      0.975083  0.098263       1.006665"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd['loss+accuracy']=pd['loss']+pd['accuracy']\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_prob=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(df['loss'])\n",
    "plt.plot(df['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
